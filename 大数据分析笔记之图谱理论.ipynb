{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "图谱理论.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Skyorca/BigDataAnalysis/blob/master/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%9B%BE%E8%B0%B1%E7%90%86%E8%AE%BA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiS3bG7Jnbwf",
        "colab_type": "text"
      },
      "source": [
        "#大数据分析课程笔记：图谱理论Spectual Graph theory\n",
        "\n",
        "本笔记的知识点如下：\n",
        "\n",
        "1.   考试重点知识总结cheat sheet\n",
        "2.   图谱理论基础及其应用\n",
        "3.   PageRank\n",
        "\n",
        "\n",
        "\n",
        "   整理by天空鱼@ict网数\n",
        "\n",
        "\n",
        "                                                                                                   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTDp8bjYodmn",
        "colab_type": "text"
      },
      "source": [
        "##part1 考试重点（按照最后一张ppt上的take away整理得到）\n",
        "\n",
        "定义图G的邻接矩阵为A, 度矩阵为D：\n",
        "\n",
        "1. $L = D-A$（对称,每行、列和为0）\n",
        "2. $L^{sym}=D^{-1/2}LD^{-1/2}$（对称）sym short for symmetry\n",
        "3. $L^{rw}=D^{-1}L=I-D^{-1}A$ （非对称）rw short for random walk\n",
        "\n",
        "以上三个都是Laplace矩阵。第一个为一般形式。第二个是对称归一化之后的形式，保证每个元素都在0~1之间。第三个是随机游走Laplace矩阵，即把$D^{-1}$看成是扩散算子，每个节点的权（没有定义就是1）以$\\frac{1}{deg(i)}$的概率分散给它的所有邻居。随机游走转移概率矩阵就是$D^{-1}A$。这个形式下的Laplace矩阵就是单位阵减去随机游走转移概率矩阵。\n",
        "\n",
        "注意$L^{sym}相似于L^{rw}$: $D^{-1/2}L^{sym}D^{1/2}=D^{-1}L=L^{rw}$。所以假设$(\\lambda, v)$是$L^{rw}$的特征值和特征向量，那么$(\\lambda, D^{1/2}v)$是$L^{sym}$的特征值和特征向量。特别地，$(0,I)$是$L^{rw}$的特征值和特征向量，$(0,D^{-1/2}I)$是$L^{sym}$的特征值和特征向量。$I$是全1向量。\n",
        "\n",
        "以下都以1、2形式的Laplace矩阵为研究对象。\n",
        "\n",
        "**特征值性质：**\n",
        "1.   Laplace矩阵是对称矩阵，特征值有n个均是实数\n",
        "2.   Laplace矩阵是半正定矩阵，特征值全部非负$0\n",
        "\\leq\\lambda_{0}\\leq\\lambda_{1}\\leq...\\lambda_{n-1}\\leq2$\n",
        "3.   Laplace矩阵最小的特征值是0，对应的特征向量是全1向量（因为L矩阵每行或者每列的和都是0）。只有在完全二分图时，最大特征值才取2。\n",
        "4. 第二小的特征值含义丰富。比如对于连通图，$\\lambda_{1}\\gt0$（其他在part2详细说明）\n",
        "\n",
        "**Laplace矩阵的二次型表示**：\n",
        "\n",
        "对于任意的向量𝑓,我们有\n",
        "$𝑓^{𝑇}𝐿𝑓=\\frac{1}{2}\\sum_{i,j=1}^{n}𝑤_{ij}(𝑓_{𝑖}−𝑓_{𝑗})^{2}$ （$𝑤_{ij}$是边权，一般是1）证明如下：\n",
        "\n",
        "\n",
        "$𝑓^{𝑇}𝐿𝑓=𝑓^{𝑇}𝐷𝑓−𝑓^{𝑇}A𝑓=\\sum_{i=1}^{n}d_{i}f_{i}^{2}−\\sum_{i,j=1}^{n}w_{ij}f_{i}f_{j}\n",
        "=\\frac{1}{2}(\\sum_{i=1}^{n}d_{i}f_{i}^{2}−2\\sum_{i,j=1}^{n}w_{ij}f_{i}f_{j}+\\sum_{j=1}^{n}d_{j}f_{j}^{2})$\n",
        "\n",
        "$=\\frac{1}{2}\\sum_{i,j=1}^{n}𝑤_{ij}(𝑓_{𝑖}−𝑓_{𝑗})^{2}$\n",
        "\n",
        "如果A是邻接矩阵，权重就都是1，可以简化为$𝑓^{𝑇}𝐿𝑓=\\frac{1}{2}\\sum_{i,j=1}^{n}(𝑓_{𝑖}−𝑓_{𝑗})^{2}$\n",
        "\n",
        "\n",
        "特别地，如果拿$L^{sym}$去写，形式为：\n",
        "$𝑓^{𝑇}𝐿^{sym}𝑓=\\frac{1}{2}\\sum_{i,j=1}^{n}(\\frac{𝑓_{𝑖}}{d_{i}}−\\frac{𝑓_{𝑗}}{d_{j}})^{2}$\n",
        "\n",
        "拿到二次型就能做优化任务。\n",
        "\n",
        "**Laplace矩阵的作用（按照ppt所介绍）：**\n",
        "1. 谱聚类（最小化图中两个节点set之间的boundary）【详细过程在part2介绍，记得看一看，和两次作业都有点关系】\n",
        "2. 二次型的最优化任务，如随机游走的迭代公式，弹簧系统的稳定形态...\n",
        "\n",
        "**对于$L=\\sum L_{G}$的理解：**\n",
        "对于对称阵A,我们可以引入正交阵U($U^{-1}=U^{T}$)把它对称对角化：$UAU^{T}=D$\n",
        "（待补充）\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir7IqYuNr5u-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "0a78467d-e48b-4249-fe25-ddb11b06fc74"
      },
      "source": [
        "#补充代码：对称正则化\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[0,1,1,1],[1,0,1,0],[1,1,0,1],[1,0,1,0]])\n",
        "print('adj matrix:\\n',A)\n",
        "D = np.diag(np.sum(A,axis=1))\n",
        "print('degree matrix:\\n',D)\n",
        "L=D-A\n",
        "print('laplacian matrix:\\n',L)\n",
        "D_ = np.diag(np.power(np.sum(A,axis=1),-0.5))\n",
        "L_sym=D_@L@D_\n",
        "print('normalized laplacian:\\n',L_sym)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adj matrix:\n",
            " [[0 1 1 1]\n",
            " [1 0 1 0]\n",
            " [1 1 0 1]\n",
            " [1 0 1 0]]\n",
            "degree matrix:\n",
            " [[3 0 0 0]\n",
            " [0 2 0 0]\n",
            " [0 0 3 0]\n",
            " [0 0 0 2]]\n",
            "laplacian matrix:\n",
            " [[ 3 -1 -1 -1]\n",
            " [-1  2 -1  0]\n",
            " [-1 -1  3 -1]\n",
            " [-1  0 -1  2]]\n",
            "normalized laplacian:\n",
            " [[ 1.         -0.40824829 -0.33333333 -0.40824829]\n",
            " [-0.40824829  1.         -0.40824829  0.        ]\n",
            " [-0.33333333 -0.40824829  1.         -0.40824829]\n",
            " [-0.40824829  0.         -0.40824829  1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo9G8v4DwjAy",
        "colab_type": "text"
      },
      "source": [
        "##part2:图谱理论详细介绍\n",
        "首先介绍为什么这个矩阵叫Laplacian，在这里会推导出为什么$L=D-A$。之后重点讨论谱聚类。谱聚类以及图嵌入是它的一个重点应用。最后稍微整理一下ppt其他的东西比如瑞利熵、$\\epsilon逼近$等。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjhkvbpWQc4H",
        "colab_type": "text"
      },
      "source": [
        "###Laplacian的由来\n",
        "这里参考知乎[拉普拉斯矩阵和拉普拉斯算子的关系](https://zhuanlan.zhihu.com/p/85287578)\n",
        "\n",
        "连续函数的梯度（一阶导）大家都会求。对于离散函数的梯度，可以把函数想象成一个数组$f$，在每个索引$i$时取值$f(i)$。这个数组的梯度$grad(f)$就是$grad(f)=f(i+1)-f(i)$,即“前减后”。因为对于梯度公式：\n",
        "\n",
        "\n",
        "*  $gradf_{i}=\\frac{f(i+\\epsilon)-f(f(i))}{\\epsilon}$\n",
        "\n",
        "对离散函数而言，$\\epsilon$就是固定间隔。这里为了方便取1。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rJbI_w4UU2u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9c3c49f9-204e-49f8-8ddd-9134113aac81"
      },
      "source": [
        "#补充代码2：求数组导数\n",
        "a = np.random.randint(0,100,10)\n",
        "grad = [a[0]]+[a[i+1]-a[i] for i in range(9)] \n",
        "a,grad"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([54, 59,  3, 68, 91, 86, 13, 28,  9, 35]),\n",
              " [54, 5, -56, 65, 23, -5, -73, 15, -19, 26])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo6bGj_6UVXK",
        "colab_type": "text"
      },
      "source": [
        "拉普拉斯算子$\\Delta$其实就是“二阶导”，即梯度组成的向量场的散度。散度的意义是含源性：如果散度＜0表示场有一个往里吸的源点；＞0表示有一个向外发散的源点；＝0无源。\n",
        "\n",
        "求离散函数的散度（二阶导）：$\\Delta f=f''(x)=f'(x)-f'(x-1)=f(x+1)+f(x-1)-2f(x)$\n",
        "\n",
        "扩展到二维平面：$\\Delta f=\\frac{\\partial^{2}f}{\\partial^{2}x}+\\frac{\\partial^{2}f}{\\partial^{2}y}=f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)$，本质上描述了如果平面上有一个扰动把(x,y)推离平衡位置，即移动到(x+1,y) or (x-1,y)or (x,y+1) or (x,y-1)，则造成的改变可能为多大。这就是拉普拉斯符号的含义。\n",
        "\n",
        "\n",
        "![离散二维函数的laplacian含义](https://pic2.zhimg.com/80/v2-f1e7c13a78ec8123200a604f2a6bb23d_hd.jpg)\n",
        "\n",
        "\n",
        "对应到图结构就很好理解：每个点天然和其他节点相连，那么在图中施加一个微扰，衡量一个节点偏离原平衡位置造成的改变大小的方法是什么呢？\n",
        "\n",
        "![图](https://pic3.zhimg.com/80/v2-6b498216b66bdc5a81d7876ca665fd32_hd.jpg)\n",
        "\n",
        "\n",
        "假设我们把整个图G编码成一个向量$f$【1】,$f_{i}$代表节点i的标量值。那么借用拉普拉斯符号的$\\Delta$表示法，当给节点i施加微扰的时候，变化为：\n",
        "\n",
        "$\\Delta f_{i}=\\sum_{j \\in N(i)}(f(i)-f(j))$\n",
        "\n",
        "即i移动到邻居N(i)中时所造成的改变。注意到：\n",
        "\n",
        "\n",
        "*  加上边$e_{ij}$的权重$w_{ij}$\n",
        "*  $\\sum_{j}w_{ij}$就是节点i的度$d_{i}$\n",
        "*  对于不在N(i)里面的j,$w_{ij}=0$的事实可以放松对j的约束\n",
        "\n",
        "我们得到：\n",
        "\n",
        "$\\Delta f_{i}=\\sum_{j}w_{ij}(f(i)-f(j))=d_{i}f_{i}-w_{i:}f$\n",
        "\n",
        "最后这个$w_{i:}f$可以看成权重矩阵的第i行与列向量f内积。之后把i按照1,...,n堆叠起来并整理成矩阵的形式(这里的权矩阵W在没有设定时退化为邻接矩阵A)：\n",
        "\n",
        "\n",
        "![推导](https://www.zhihu.com/equation?tex=%5Ceqalign%7B+++%26+%5CDelta+%7Bf%7D+%3D%5Cleft%28+%7B%5Cmatrix%7B++++%7B%5CDelta+%7Bf_1%7D%7D++%5Ccr++++++%5Cvdots+++%5Ccr+++++%7B%5CDelta+%7Bf_N%7D%7D++%5Ccr++++%7D+%7D+%5Cright%29+%3D+%5Cleft%28+%7B%5Cmatrix%7B++++%7B%7Bd_1%7D%7Bf_1%7D+-+%7Bw_%7B1%3A%7D%7Df%7D++%5Ccr++++++%5Cvdots+++%5Ccr+++++%7B%7Bd_N%7D%7Bf_N%7D+-+%7Bw_%7BN%3A%7D%7Df%7D++%5Ccr++++%7D+%7D+%5Cright%29++%5Ccr++++%26++%3D%5Cleft%28+%7B%5Cmatrix%7B++++%7B%7Bd_1%7D%7D+%26++%5Ccdots++%26+0++%5Ccr++++++%5Cvdots++%26++%5Cddots++%26++%5Cvdots+++%5Ccr+++++0+%26++%5Ccdots++%26+%7B%7Bd_N%7D%7D++%5Ccr++++%7D+%7D+%5Cright%29++f+-+%5Cleft%28+%7B%5Cmatrix%7B++++%7B%7Bw_%7B1%3A%7D%7D%7D++%5Ccr++++++%5Cvdots+++%5Ccr+++++%7B%7Bw_%7BN%3A%7D%7D%7D++%5Ccr++++%7D+%7D+%5Cright%29f++++%5Ccr+%26++%3D+diag%28%7Bd_i%7D%29f+-+Wf++%5Ccr++++%26++%3D+%28D+-W%29f++%5Ccr++++%26++%3D+Lf+%5Ccr%7D+)\n",
        "\n",
        "\n",
        "**拉普拉斯矩阵中的第$i$行实际上反应了第$i$个节点在受扰而向他的所有邻居产生移动倾向时所产生的增益累积**。直观上来讲，图拉普拉斯反映了当我们在节点$i$上施加一个势，这个势以哪个方向能够多顺畅的流向其他节点。谱聚类中的拉普拉斯矩阵可以理解为是对图的一种矩阵表示形式。\n",
        "\n",
        "你看，拉普拉斯符号和拉普拉斯矩阵被奇妙地统一了！How sexy it is!\n",
        "---\n",
        "\n",
        "\n",
        "【1】：其实是一种图嵌入。实际上不可能把整张图编码为一个向量，基本上都是编码成一个矩阵，那么每个节点$f_{i}$就是一个列向量，$\\Delta f_{i}$也是。上文中我们一直默认它们都是标量值，容易理解；但是如果把它们当成向量和矩阵回头看一下形式上没什么变化。只不过注意$\\Delta f$那里是一个矩阵，而横向堆叠$\\Delta f_{i}$时实际上是它的转置$\\Delta f_{i}^{T}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmU44LHZMIr3",
        "colab_type": "text"
      },
      "source": [
        "###谱聚类\n",
        "\n",
        "参考链接：\n",
        "\n",
        "[谱聚类1](https://www.cnblogs.com/pinard/p/6221564.html)\n",
        "\n",
        "[谱聚类2](https://zhuanlan.zhihu.com/p/73887392)\n",
        "\n",
        "\n",
        "\n",
        "因为东西太多所以参考博客内容就好。下面只给出概括性的东西，可以先看完博客再回来梳理一遍：\n",
        "\n",
        "\n",
        "\n",
        "> 什么是谱聚类？\n",
        "谱聚类首先把数据看成高维空间中的点，这些数据可以天然成图或者可以构造成图关系（比如社交网络图等），则对连边有权重矩阵W;如果W有定义则为定义中的数值（比如社交网络中两个节点的共同好友数等），如果没有，则定义相似矩阵S,(上面链接1有讲)，用S代替W。\n",
        "\n",
        "定义好权重矩阵W之后，谱聚类的目的是形成簇，每个簇里面的边权重和大，簇之间的边权重和小（即前文所述最小化Boundary）。我们的目的就是找一些割，能把图割成一些这样的簇。这里会有一些问题：如何寻找这些割？要切成多少簇？如何评价结果好坏？等等。\n",
        "\n",
        "\n",
        "\n",
        "> 谱聚类的一般过程（背后的证明过程在链接1里）：\n",
        "1.   定义好W，形成laplacian矩阵$L^{rw}$or$L^{sym}$\n",
        "2.   通过找到laplacian矩阵的最小的k个特征值，可以得到对应的k个特征向量，这k个特征向量组成一个nxk维度的矩阵。每一行就是数据点从n维空间映射到k维的新表示。（两次作业均涉及到数据点在新空间中的表示）\n",
        "3.   再通过K-means等传统聚类手段聚类一下形成若干簇。\n",
        "\n",
        "Q1: 为什么要用laplacian矩阵而不是直接用权重矩阵W？\n",
        "\n",
        "A1: 看一下博客1背后的理论推导，就知道使用laplacian矩阵是很自然得到的结论。另外可以从PCA的角度理解：把W看成PCA里面的协方差矩阵，它的目的是得到最大的特征值和特征向量；而谱聚类需要的是最小的，则通过D-W恰好反过来。下图是一种从连通性出发的解释，出自《A Tutorial on Spectral Clustering》。\n",
        "\n",
        "![这里还有一种基于连通性的解释](https://pic1.zhimg.com/80/v2-0ae8205e7f9a25fec4cb25ac059964b0_hd.jpg)\n",
        "\n"
      ]
    }
  ]
}