{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "图谱理论.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Skyorca/BigDataAnalysis/blob/master/%E5%9B%BE%E8%B0%B1%E7%90%86%E8%AE%BA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiS3bG7Jnbwf",
        "colab_type": "text"
      },
      "source": [
        "#大数据分析课程笔记：图谱理论Spectual Graph theory\n",
        "\n",
        "本笔记的知识点如下：\n",
        "\n",
        "1.   考试重点知识总结cheat sheet\n",
        "2.   图谱理论基础及其应用\n",
        "3.   PageRank\n",
        "\n",
        "\n",
        "\n",
        "   整理by天空鱼@ict网数\n",
        "\n",
        "\n",
        "                                                                                                   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTDp8bjYodmn",
        "colab_type": "text"
      },
      "source": [
        "##part1 考试重点（按照最后一张ppt上的take away整理得到）\n",
        "\n",
        "定义图G的邻接矩阵为A, 度矩阵为D：\n",
        "\n",
        "1. $L = D-A$（对称,每行、列和为0）\n",
        "2. $L^{sym}=D^{-1/2}LD^{-1/2}$（对称）sym short for symmetry\n",
        "3. $L^{rw}=D^{-1}L=I-D^{-1}A$ （非对称）rw short for random walk\n",
        "\n",
        "以上三个都是Laplace矩阵。第一个为一般形式。第二个是对称归一化之后的形式，保证每个元素都在0~1之间。第三个是随机游走Laplace矩阵，即把$D^{-1}$看成是扩散算子，每个节点的权（没有定义就是1）以$\\frac{1}{deg(i)}$的概率分散给它的所有邻居。随机游走转移概率矩阵就是$D^{-1}A$。这个形式下的Laplace矩阵就是单位阵减去随机游走转移概率矩阵。\n",
        "\n",
        "注意$L^{sym}相似于L^{rw}$: $D^{-1/2}L^{sym}D^{1/2}=D^{-1}L=L^{rw}$。所以假设$(\\lambda, v)$是$L^{rw}$的特征值和特征向量，那么$(\\lambda, D^{1/2}v)$是$L^{sym}$的特征值和特征向量。特别地，$(0,I)$是$L^{rw}$的特征值和特征向量，$(0,D^{-1/2}I)$是$L^{sym}$的特征值和特征向量。$I$是全1向量。\n",
        "\n",
        "\n",
        "\n",
        "> 书后习题2：证明$D^{-1}A$有一个特征值是1。\n",
        "解答：特征值1对应的对应向量是$I$,因为转移概率矩阵每行的和是1（通过$D^{-1}$对A做的归一化）\n",
        "\n",
        "\n",
        "\n",
        "以下都以1、2形式的Laplace矩阵为研究对象。\n",
        "\n",
        "**特征值性质：**\n",
        "1.   Laplace矩阵是对称矩阵，特征值有n个均是实数\n",
        "2.   Laplace矩阵是半正定矩阵，特征值全部非负$0\n",
        "\\leq\\lambda_{0}\\leq\\lambda_{1}\\leq...\\lambda_{n-1}\\leq2$\n",
        "3.   Laplace矩阵最小的特征值是0，对应的特征向量是全1向量（因为L矩阵每行或者每列的和都是0）。只有在完全二分图时，最大特征值才取2。\n",
        "4. 第二小的特征值含义丰富。比如对于连通图，$\\lambda_{1}\\gt0$（其他在part2详细说明）\n",
        "\n",
        "\n",
        "\n",
        "**Laplace矩阵的二次型表示**：\n",
        "\n",
        "对于任意的向量𝑓,我们有\n",
        "$𝑓^{𝑇}𝐿𝑓=\\frac{1}{2}\\sum_{i,j=1}^{n}𝑤_{ij}(𝑓_{𝑖}−𝑓_{𝑗})^{2}$ （$𝑤_{ij}$是边权，一般是1）证明如下：\n",
        "\n",
        "\n",
        "$𝑓^{𝑇}𝐿𝑓=𝑓^{𝑇}𝐷𝑓−𝑓^{𝑇}A𝑓=\\sum_{i=1}^{n}d_{i}f_{i}^{2}−\\sum_{i,j=1}^{n}w_{ij}f_{i}f_{j}\n",
        "=\\frac{1}{2}(\\sum_{i=1}^{n}d_{i}f_{i}^{2}−2\\sum_{i,j=1}^{n}w_{ij}f_{i}f_{j}+\\sum_{j=1}^{n}d_{j}f_{j}^{2})$\n",
        "\n",
        "$=\\frac{1}{2}\\sum_{i,j=1}^{n}𝑤_{ij}(𝑓_{𝑖}−𝑓_{𝑗})^{2}$\n",
        "\n",
        "如果A是邻接矩阵，权重就都是1，可以简化为$𝑓^{𝑇}𝐿𝑓=\\frac{1}{2}\\sum_{i,j=1}^{n}(𝑓_{𝑖}−𝑓_{𝑗})^{2}$\n",
        "\n",
        "\n",
        "特别地，如果拿$L^{sym}$去写，形式为：\n",
        "$𝑓^{𝑇}𝐿^{sym}𝑓=\\frac{1}{2}\\sum_{i,j=1}^{n}(\\frac{𝑓_{𝑖}}{d_{i}}−\\frac{𝑓_{𝑗}}{d_{j}})^{2}$\n",
        "\n",
        "拿到二次型就能做优化任务。\n",
        "\n",
        "\n",
        "\n",
        "> 书后习题6：证明无向图的laplacian矩阵特征值均非负\n",
        "解答：首先证明L矩阵半正定。写出它的二次型表示可以轻易得到$x^{T}Lx\\geq 0$。根据半正定的定义L就是半正定矩阵。接下来从几何和代数两个角度说明半正定矩阵特征值非负。\n",
        "*   几何角度：考察半正定矩阵$H$作用在向量$x$上得到的新向量$HX$: $<x,HX>=x^{T}Hx\\geq 0$(内积运算)，说明$HX$和$x$的夹角$\\leq$90°。再考察特征值和特征向量：$Hv=\\lambda v$, 新向量$Hv$即$\\lambda v$和$v$的夹角也应该$\\leq$90°,如果$\\lambda \\lt 0$则把原向量翻转，夹角$\\gt 90°$,矛盾。\n",
        "*   代数角度： $Hv=\\lambda v \\Rightarrow v^{T}Hv=v^{T}\\lambda v=\\lambda v^{T}v \\geq 0$。而 $v^{T}v \\geq 0$，所以$\\lambda \\geq 0$\n",
        "\n",
        "\n",
        "**Laplace矩阵的作用（按照ppt所介绍）：**\n",
        "1. 谱聚类（最小化图中两个节点set之间的boundary）【详细过程在part2介绍，记得看一看，和两次作业都有点关系】\n",
        "2. 二次型的最优化任务，如随机游走的迭代公式，弹簧系统的稳定形态...\n",
        "\n",
        "**PageRank**:\n",
        "考试重点。参考笔记第三部分。要掌握更新公式和手动计算更新过程。\n",
        "\n",
        "\n",
        "**补充：对于$L=\\sum L_{G}$的理解**\n",
        "对于对称阵A,我们可以引入正交阵U($U^{-1}=U^{T}$)把它对称对角化：$UAU^{T}=D$\n",
        "（待补充）\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir7IqYuNr5u-",
        "colab_type": "code",
        "outputId": "0a78467d-e48b-4249-fe25-ddb11b06fc74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "#补充代码：对称正则化\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[0,1,1,1],[1,0,1,0],[1,1,0,1],[1,0,1,0]])\n",
        "print('adj matrix:\\n',A)\n",
        "D = np.diag(np.sum(A,axis=1))\n",
        "print('degree matrix:\\n',D)\n",
        "L=D-A\n",
        "print('laplacian matrix:\\n',L)\n",
        "D_ = np.diag(np.power(np.sum(A,axis=1),-0.5))\n",
        "L_sym=D_@L@D_\n",
        "print('normalized laplacian:\\n',L_sym)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adj matrix:\n",
            " [[0 1 1 1]\n",
            " [1 0 1 0]\n",
            " [1 1 0 1]\n",
            " [1 0 1 0]]\n",
            "degree matrix:\n",
            " [[3 0 0 0]\n",
            " [0 2 0 0]\n",
            " [0 0 3 0]\n",
            " [0 0 0 2]]\n",
            "laplacian matrix:\n",
            " [[ 3 -1 -1 -1]\n",
            " [-1  2 -1  0]\n",
            " [-1 -1  3 -1]\n",
            " [-1  0 -1  2]]\n",
            "normalized laplacian:\n",
            " [[ 1.         -0.40824829 -0.33333333 -0.40824829]\n",
            " [-0.40824829  1.         -0.40824829  0.        ]\n",
            " [-0.33333333 -0.40824829  1.         -0.40824829]\n",
            " [-0.40824829  0.         -0.40824829  1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo9G8v4DwjAy",
        "colab_type": "text"
      },
      "source": [
        "##part2:图谱理论详细介绍\n",
        "首先介绍为什么这个矩阵叫Laplacian，在这里会推导出为什么$L=D-A$。之后重点讨论谱聚类。谱聚类以及图嵌入是它的一个重点应用。最后稍微整理一下ppt其他的东西比如瑞利熵、$\\epsilon逼近$等。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjhkvbpWQc4H",
        "colab_type": "text"
      },
      "source": [
        "###1. Laplacian的由来\n",
        "这里参考知乎[拉普拉斯矩阵和拉普拉斯算子的关系](https://zhuanlan.zhihu.com/p/85287578)\n",
        "\n",
        "连续函数的梯度（一阶导）大家都会求。对于离散函数的梯度，可以把函数想象成一个数组$f$，在每个索引$i$时取值$f(i)$。这个数组的梯度$grad(f)$就是$grad(f)=f(i+1)-f(i)$,即“前减后”。因为对于梯度公式：\n",
        "\n",
        "\n",
        "*  $gradf_{i}=\\frac{f(i+\\epsilon)-f(f(i))}{\\epsilon}$\n",
        "\n",
        "对离散函数而言，$\\epsilon$就是固定间隔。这里为了方便取1。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rJbI_w4UU2u",
        "colab_type": "code",
        "outputId": "9c3c49f9-204e-49f8-8ddd-9134113aac81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#补充代码2：求数组导数\n",
        "a = np.random.randint(0,100,10)\n",
        "grad = [a[0]]+[a[i+1]-a[i] for i in range(9)] \n",
        "a,grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([54, 59,  3, 68, 91, 86, 13, 28,  9, 35]),\n",
              " [54, 5, -56, 65, 23, -5, -73, 15, -19, 26])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo6bGj_6UVXK",
        "colab_type": "text"
      },
      "source": [
        "拉普拉斯算子$\\Delta$其实就是“二阶导”，即梯度组成的向量场的散度。散度的意义是含源性：如果散度＜0表示场有一个往里吸的源点；＞0表示有一个向外发散的源点；＝0无源。\n",
        "\n",
        "求离散函数的散度（二阶导）：$\\Delta f=f''(x)=f'(x)-f'(x-1)=f(x+1)+f(x-1)-2f(x)$\n",
        "\n",
        "扩展到二维平面：$\\Delta f=\\frac{\\partial^{2}f}{\\partial^{2}x}+\\frac{\\partial^{2}f}{\\partial^{2}y}=f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)$，本质上描述了如果平面上有一个扰动把(x,y)推离平衡位置，即移动到(x+1,y) or (x-1,y)or (x,y+1) or (x,y-1)，则造成的改变可能为多大。这就是拉普拉斯符号的含义。\n",
        "\n",
        "\n",
        "![离散二维函数的laplacian含义](https://pic2.zhimg.com/80/v2-f1e7c13a78ec8123200a604f2a6bb23d_hd.jpg)\n",
        "\n",
        "\n",
        "对应到图结构就很好理解：每个点天然和其他节点相连，那么在图中施加一个微扰，衡量一个节点偏离原平衡位置造成的改变大小的方法是什么呢？\n",
        "\n",
        "![图](https://pic3.zhimg.com/80/v2-6b498216b66bdc5a81d7876ca665fd32_hd.jpg)\n",
        "\n",
        "\n",
        "假设我们把整个图G编码成一个向量$f$【1】,$f_{i}$代表节点i的标量值。那么借用拉普拉斯符号的$\\Delta$表示法，当给节点i施加微扰的时候，变化为：\n",
        "\n",
        "$\\Delta f_{i}=\\sum_{j \\in N(i)}(f(i)-f(j))$\n",
        "\n",
        "即i移动到邻居N(i)中时所造成的改变。注意到：\n",
        "\n",
        "\n",
        "*  加上边$e_{ij}$的权重$w_{ij}$\n",
        "*  $\\sum_{j}w_{ij}$就是节点i的度$d_{i}$\n",
        "*  对于不在N(i)里面的j,$w_{ij}=0$的事实可以放松对j的约束\n",
        "\n",
        "我们得到：\n",
        "\n",
        "$\\Delta f_{i}=\\sum_{j}w_{ij}(f(i)-f(j))=d_{i}f_{i}-w_{i:}f$\n",
        "\n",
        "最后这个$w_{i:}f$可以看成权重矩阵的第i行与列向量f内积。之后把i按照1,...,n堆叠起来并整理成矩阵的形式(这里的权矩阵W在没有设定时退化为邻接矩阵A)：\n",
        "\n",
        "\n",
        "![推导](https://www.zhihu.com/equation?tex=%5Ceqalign%7B+++%26+%5CDelta+%7Bf%7D+%3D%5Cleft%28+%7B%5Cmatrix%7B++++%7B%5CDelta+%7Bf_1%7D%7D++%5Ccr++++++%5Cvdots+++%5Ccr+++++%7B%5CDelta+%7Bf_N%7D%7D++%5Ccr++++%7D+%7D+%5Cright%29+%3D+%5Cleft%28+%7B%5Cmatrix%7B++++%7B%7Bd_1%7D%7Bf_1%7D+-+%7Bw_%7B1%3A%7D%7Df%7D++%5Ccr++++++%5Cvdots+++%5Ccr+++++%7B%7Bd_N%7D%7Bf_N%7D+-+%7Bw_%7BN%3A%7D%7Df%7D++%5Ccr++++%7D+%7D+%5Cright%29++%5Ccr++++%26++%3D%5Cleft%28+%7B%5Cmatrix%7B++++%7B%7Bd_1%7D%7D+%26++%5Ccdots++%26+0++%5Ccr++++++%5Cvdots++%26++%5Cddots++%26++%5Cvdots+++%5Ccr+++++0+%26++%5Ccdots++%26+%7B%7Bd_N%7D%7D++%5Ccr++++%7D+%7D+%5Cright%29++f+-+%5Cleft%28+%7B%5Cmatrix%7B++++%7B%7Bw_%7B1%3A%7D%7D%7D++%5Ccr++++++%5Cvdots+++%5Ccr+++++%7B%7Bw_%7BN%3A%7D%7D%7D++%5Ccr++++%7D+%7D+%5Cright%29f++++%5Ccr+%26++%3D+diag%28%7Bd_i%7D%29f+-+Wf++%5Ccr++++%26++%3D+%28D+-W%29f++%5Ccr++++%26++%3D+Lf+%5Ccr%7D+)\n",
        "\n",
        "\n",
        "**拉普拉斯矩阵中的第$i$行实际上反应了第$i$个节点在受扰而向他的所有邻居产生移动倾向时所产生的增益累积**。直观上来讲，图拉普拉斯反映了当我们在节点$i$上施加一个势，这个势以哪个方向能够多顺畅的流向其他节点。谱聚类中的拉普拉斯矩阵可以理解为是对图的一种矩阵表示形式。\n",
        "\n",
        "你看，拉普拉斯符号和拉普拉斯矩阵被奇妙地统一了！How sexy it is!\n",
        "---\n",
        "\n",
        "\n",
        "【1】：其实是一种图嵌入。实际上不可能把整张图编码为一个向量，基本上都是编码成一个矩阵，那么每个节点$f_{i}$就是一个列向量，$\\Delta f_{i}$也是。上文中我们一直默认它们都是标量值，容易理解；但是如果把它们当成向量和矩阵回头看一下形式上没什么变化。只不过注意$\\Delta f$那里是一个矩阵，而横向堆叠$\\Delta f_{i}$时实际上是它的转置$\\Delta f_{i}^{T}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmU44LHZMIr3",
        "colab_type": "text"
      },
      "source": [
        "###2. 谱聚类\n",
        "\n",
        "参考链接：\n",
        "\n",
        "[谱聚类1](https://www.cnblogs.com/pinard/p/6221564.html)\n",
        "\n",
        "[谱聚类2](https://zhuanlan.zhihu.com/p/73887392)\n",
        "\n",
        "[谱聚类3](https://tianchi.aliyun.com/forum/postDetail?postId=77648)\n",
        "\n",
        "\n",
        "因为东西太多所以参考博客内容就好。下面只给出概括性的东西，可以先看完博客再回来梳理一遍：\n",
        "\n",
        "\n",
        "\n",
        "> 什么是谱聚类？\n",
        "谱聚类首先把数据看成高维空间中的点，这些数据可以天然成图或者可以构造成图关系（比如社交网络图等），则对连边有权重矩阵W;如果W有定义则为定义中的数值（比如社交网络中两个节点的共同好友数等），如果没有，则定义相似矩阵S,(上面链接1有讲)，用S代替W。\n",
        "\n",
        "定义好权重矩阵W之后，谱聚类的目的是形成簇，每个簇里面的边权重和大，簇之间的边权重和小（即前文所述最小化Boundary）。我们的目的就是找一些割，能把图割成一些这样的簇。这里会有一些问题：如何寻找这些割？要切成多少簇？如何评价结果好坏？等等。\n",
        "\n",
        "\n",
        "\n",
        "> 谱聚类的一般过程（背后的证明过程在链接1里）：\n",
        "1.   定义好W，形成laplacian矩阵$L^{rw}$or$L^{sym}$\n",
        "2.   通过找到laplacian矩阵的最小的k个特征值，可以得到对应的k个特征向量，这k个特征向量组成一个nxk维度的矩阵。每一行就是数据点从n维空间映射到k维的新表示。（两次作业均涉及到数据点在新空间中的表示）\n",
        "3.   再通过K-means等传统聚类手段聚类一下形成若干簇。\n",
        "\n",
        "Q1: 为什么要用laplacian矩阵而不是直接用权重矩阵W？\n",
        "\n",
        "A1: 看一下博客1背后的理论推导，就知道使用laplacian矩阵是很自然得到的结论。另外可以从PCA的角度理解：把W看成PCA里面的协方差矩阵，它的目的是得到最大的特征值和特征向量；而谱聚类需要的是最小的，则通过D-W恰好反过来。下图是一种从连通性出发的解释，出自《A Tutorial on Spectral Clustering》。\n",
        "\n",
        "![这里还有一种基于连通性的解释](https://pic1.zhimg.com/80/v2-0ae8205e7f9a25fec4cb25ac059964b0_hd.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJlb4WEsbhJ2",
        "colab_type": "text"
      },
      "source": [
        "##Pagerank\n",
        "https://nlp.stanford.edu/IR-book/html/htmledition/pagerank-1.html\n",
        "\n",
        "\n",
        "\b\bPagerank把网页按照链接关系组织成的有向图的每个节点标上重要程度score，是一种无监督的方法；它只依赖于图结构本身，即网页之间的链接结构决定网页的重要性。\n",
        "###1.原始的pagerank\n",
        "可以把pagerank看成n个节点的有向图上的随机游走，设置一个节点为初始点，按照转移概率转移到它的任一个邻居，$i->j转移概率:p_{ij}=\\frac{1}{d_{i}}$。对于任意点j，则接受它的所有邻居向他转移，此时定义从邻居转移来的总和为$r_{j}$, 即任一节点的rank-score。\n",
        "\n",
        "$r_{j}=\\sum_{i\\rightarrow j}\\frac{r_{i}}{d_{i}}$.............*pagerank更新公式1*\n",
        "\n",
        "$\\sum_{j}r_{j}=1$：所有节点的rank-score总和为1（人为约束,在一开始的版本里rank-score总和为页数，但后面修改为这个）\n",
        "\n",
        "如果以泛洪式（Flow）的观点来看的话，就相当于每个节点i把自己的rank-score $r_{i}$平均分给它的所有邻居$i\\rightarrow j$。\n",
        "\n",
        "按照线性方程组的观点考察更新公式1，j=1,...,n，就是一个n个变量n个方程但没有常量的方程组，是没有唯一解的；但加上总和为1的约束就能解出唯一解。但是高斯消元法对于大型方程组太慢了。我们可以转换视角以迭代的观点看更新公式1。\n",
        "\n",
        "定义矩阵M：$M_{ij}=\\frac{1}{d_{j}}$，即从j转移到i的概率。M是column-stochastic matrix，因为每列和是1（每个节点均匀分散的转移概率之和）。每个节点的rank-score $r_{i}$组成了向量$r$。M又称作stochastic adj matrix。\n",
        "\n",
        "$r^{t+1}=M*r^{t}$.............*pagerank更新公式2*\n",
        "\n",
        "暂时认为迭代公式一定收敛，假设收敛到$r^{*}$，收敛后有$r^{*}=Mr^{*}$，则可以看出$r^{*}$是M特征值为1对应的一个特征向量。实际上，矩阵M的作用就是在空间里将向量$r$推向M的那个特征向量。下图中，原始向量是v，特征向量是s1，动态过程在图中被刻画。\n",
        "![空间中的表现](https://www.dhruvonmath.com/public/images/pagerank/eigen_process.png)\n",
        "\n",
        "###3. 对pagerank的修正\n",
        "\n",
        "图结构会有两个例外：1）某节点只有入链没有出链 2）出现一个团，互相循环引用。它们会影响算法的收敛和最终的迭代结果。解决办法是：首先把1）中的节点增加指向所有节点的链接（包括自己），之后对所有节点增加概率$\\beta$的随机跳转(teleport)：以$\\beta$的概率沿着本来的链接，以1-$\\beta$的概率随机跳转到任意一个节点。这样如果陷入互相引用的团，会在几次迭代后跳出。这两个解决办法通过引入能量均分矩阵:$\\frac{1}{N}$解决。$\\frac{1}{N}$是n*n矩阵，每个元素都是$\\frac{1}{n}$。这样，以上两个更新公式可以修正为：\n",
        "\n",
        "$r_{j}=\\beta \\sum_{i\\rightarrow j}\\frac{r_{i}}{d_{i}}+(1-\\beta)\\frac{1}{n}$.............*pagerank更新公式3*\n",
        "\n",
        "$A=\\beta M+(1-\\beta)\\frac{1}{N}$............Google matrix\n",
        "\n",
        "$r^{t+1}=\\beta Mr^{t}+(1-\\beta)\\frac{1}{N}=Ar^{t}$.............*pagerank更新公式4*\n",
        "\n",
        "r的初始值为均匀分布：$r^{0}_{i}=\\frac{1}{n}$\n",
        "\n",
        "**要学会按照公式3，4手动计算迭代过程。**\n",
        "\n",
        "修正后，图结构里每个节点都有入链和出链，并且每个节点都指向图中所有节点（包括自己）。每条链都带有转移概率。\n",
        "\n",
        "###4\n",
        "以上实际上提到了三种求解pagerank score的做法：第一个是解线性方程组，因为高斯消元法在大型方程组里效率不高而舍弃；第二个是矩阵M求解特征值为1对应的特征向量，但效率也不高；第三个就是迭代法，更新公式很重要。\n",
        "\n",
        "还可以继续考察很多东西，比如课件上的personalized pagerank，simrank，pagerank实际使用时的情况，等等。以及课件没提到的从Markov链观点刻画pagerank的过程，等等。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhLJr3cDRXIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9de64563-61d3-496d-a72c-a0c7d0b72f97"
      },
      "source": [
        "#补充代码3：非矩阵形式的pagerank,即更新公式3 矩阵形式更简单\n",
        "\n",
        "PR = {}\n",
        "# ------ init PR scores -------\n",
        "PR['Yahoo'] = 1/3.0 #入链来自Yahoo, Amazon\n",
        "PR['Amazon'] = 1/3.0 #入链来自Yahoo\n",
        "PR['Microsoft'] = 1/3.0 #入链来自Amazon Microsoft\n",
        "\n",
        "# ------ set the random jump probabilities and 1/N\n",
        "beta = 0.8\n",
        "n = 1/3.0\n",
        "\n",
        "# ------- iteratively update PR scores ---------\n",
        "num = 100\n",
        "for i in range(num):\n",
        "    PR_new = PR.copy()\n",
        "    PR_new['Yahoo'] = beta*((1/2)*PR['Yahoo'] + (1/2)*PR['Amazon']) + (1-beta)*n\n",
        "    PR_new['Amazon'] = beta*((1/2)*PR['Yahoo']) + (1-beta)*n\n",
        "    PR_new['Microsoft'] = beta*((1/2)*PR['Amazon']+PR['Microsoft']) + (1-beta)*n\n",
        "\n",
        "    PR = PR_new\n",
        "\n",
        "print(PR)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Yahoo': 0.21212121212121213, 'Amazon': 0.1515151515151515, 'Microsoft': 0.6363636363636364}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}